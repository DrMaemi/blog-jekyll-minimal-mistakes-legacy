---
title: '[회고] 소프트웨어 마에스트로 12기 프로젝트 i-mind'
uml: true
author_profile: true
toc_label: '[회고] 소프트웨어 마에스트로 12기 프로젝트 i-mind'
---

<figure data-ke-type="opengraph"><a href="https://github.com/DrMaemi/i-mind-net" data-source-url="https://github.com/DrMaemi/i-mind-net">
<div class="og-image" style="background-image: url('https://img.youtube.com/vi/VDkS72N5ALQ/0.jpg');"></div>
<div class="og-text">
<p class="og-title">i-mind-net</p>
<p class="og-desc">i-mind-net은 서비스 i-mind에서 영상을 입력받으면 영상 내 객체 별 행동과 표정(감정) 정보를 메타데이터로 추출하는 기능을 수행합니다. 해당 데이터를 통해 영상 내 등장했던 객체를 구별하여 어느 시간대에 어떤 행동과 감정을 가졌는지 등을 알 수 있습니다.</p>
<p class="og-host">https://github.com/drmaemi</p></div></a></figure>

## 들어가며, 관심사의 변화와 회고 주제
저는 지난 해 2021년 상반기에 대학교 마지막 3학점을 수강하면서 SW 개발 대외활동을 하기 위해 조사를 하던 중 과학기술정보통신부에서 주관하는 SW 마에스트로 12기에 지원했고 선발되어 활동할 수 있었습니다. i-mind는 12기 본 과정에서 팀원 2명을 포함한 총 3인 팀으로 진행한 프로젝트로, 당시 코로나로 인한 아동우울증 증가로 아동심리상담의 수요가 크게 증가했지만 상담센터 방문 부담과 비용 등 여러 문제로 인해 수요를 만족시키지 못하고 있다고 판단하여 이를 해결할 수 있는 온라인 웹 서비스를 개발하고자 시작하게 됐습니다.

프로젝트를 진행하면서 저는 AI 기능을 개발하고 성능을 개선시키는 작업을 진행하면서 팀원과 함께 클라우드 아키텍처를 설계하여 서비스 운영 환경을 구축하는 역할을 수행했습니다. 이 당시 AI 기술에 관심이 많았고 개발 시 겪었던 문제의 반 정도는 AI 기능 개발 환경 기술 스택, 컴퓨터 비전 이론과 관련된 문제였으나 제가 최종적으로 지향하는 커리어 시작 목표는 백엔드 서버 엔지니어이기 때문에 위 문제와 관련된 회고는 나중에 하기로 하고, 우선 백엔드 기술과 직접적으로 연관이 있었던 이슈들인 AWS Lambda를 이용한 Event-driven 서버리스 아키텍처 설계, 프로파일링 도구를 이용한 처리 성능 하락 원인 파악과 데이터 배치 처리를 통해 CPU-GPU 메모리 간 Context-switch를 최소화한 성능 향상 경험에 대해 서술해나갈 예정입니다. 여기에 추가적으로 프로젝트 진행 시 아쉬웠던 점, 향후 방향 등에 대해서도 작성할 예정입니다.

## AWS Lambda를 이용한 Event-driven 서버리스 아키텍처 설계
AI 모델로 영상의 메타 데이터를 추출하는 기능을 개발하고 이를 서비스로 배포하고자 했을 때 처음에는 파이썬의 Flask나 Django로 웹 서버를 클라우드에 배포하고 AWS ELB나 EKS를 이용한 아키텍처를 구성하여 서비스를 운영하고자 했는데 AI 기능이 동작할 서버의 스펙이 GPU를 활용해야 할 고성능 서버였기 때문에 비용 문제가 예상됐었습니다. 이를 해결하기 위해 AWS Lambda를 사용했었습니다. 기존에 개발해뒀던 AI 기능을 라이브러리로 패키징해서 EC2 이미지로 배포해두고, 서비스 운영 시 API Gateway로부터 요청이 있을 때에만 AWS Lambda가 해당 EC2 이미지를 이용해 EC2 인스턴스를 생성하여 요청을 처리하고 생성한 데이터를 AWS S3에 저장하는 형태로 아키텍처를 구성했습니다.

<div class="mxgraph" style="max-width:100%; margin: auto;" data-mxgraph="{&quot;highlight&quot;:&quot;#0000ff&quot;,&quot;lightbox&quot;:false,&quot;nav&quot;:true,&quot;edit&quot;:&quot;_blank&quot;,&quot;xml&quot;:&quot;&lt;mxfile host=\&quot;app.diagrams.net\&quot; modified=\&quot;2022-06-26T12:57:29.790Z\&quot; agent=\&quot;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36\&quot; etag=\&quot;XqSajipAtzOtZiRWgeaU\&quot; version=\&quot;20.0.3\&quot; type=\&quot;google\&quot;&gt;&lt;diagram id=\&quot;J5H8LLKWrVG2AZj109Ky\&quot; name=\&quot;AWS Architecture\&quot;&gt;7Vtbb6s4EP41eTwIX8DwmKTN2SP1rKqNVt23ygUnQSUYEadJ++vXBkMAk9M0S9JkT3tRYPB15vtmxoMyQOPl9ntG08VPHrJ4AO1wO0A3AwgBhnCg/uzwtZAQBxeCeRaFutFOMI3emBbaWrqOQrZqNBScxyJKm8KAJwkLRENGs4xvms1mPG7OmtI5MwTTgMam9CEKxaKQepDs5H+waL4oZwauXzxZ0rKx3slqQUO+qYnQ7QCNM85FcbXcjlmslFfqpeg32fO0WljGEnFIh9U3xFLBH4fZnz9f/vrngT7cPXzTo7zQeK03PHyYSsEdXT6FVC9cvJba4GsRRwkbV8q2B2gU0tWChfrmhWUikuq7o08svuerSEQ8kc+euBB8WWswjKO5eiB4KqVU3wVyMyyTgoVYxvIeyEupuFTNv9zOFcYsulkhK84X+DhbJ0Exw2gWxfGYxzzLl4omjucgLOWyUxjJYctnCU/kaCO9cbkatt2rUVDZSQKc8SUT2atsojsgR5tWY9v3LeIUks0OKyUgFjWYEM0KqtE5r8beGVBeaBt+wJ7QtOeSvkn9QPt2DA17rp6ZCBbadJ3GnfGd5qRW5e9ELWefVlci48+s1mGW/5jmqQY6CD3LKAzVmg30VLD6FYA2i0iwaUoDteuNxJDel/Y2oNznVCsmB51sHiVzeYe6IYitNOPhOhA/ghx+aVZcNNuwAPaDNew3seY5XVgjtok1YNsnAhsywGYALOPrJMzNq5RKs6Cu88r0oIUPDaY9GFPd7qmQZlbq9uSEciE2yD9LBJa+uiftI0Qa2i/1XFM9xNjUfeUhetc9NnSvgpZCbNsEcteiyQeDK21WVXTL2Cp6o0/5UMpiKY8SkW/FGQ2cGzXWWkiavlW0afBfG6zDtHX6uQb9eqAL8BsGw6bBvA6uoFOZy3mfKiwJhyphaQK94kfNfjtS2R2qLAZmoZHXtHQnJ+frLGC/WHWZcNFszsR7Uce0RU3ZToeyS1nGYiqil+ZyuyygZ7hXINyZuiKjNrVhxGKbulc9QWoNhGBzIOC1Bir0YAyUA6La9vEYcb9ysf5yMXAByRj5SsZ+j2QMXEI25l1liHEPDDHkkkIM6ivEwDOHGP8rxPQXYi7hvF+Wyb5izP89xlzEiR+Y9cJrCDL+gUEG2N0m+Zwog/uKMujMUQaYVci/V5KjhjPSJKOBUF6ik7Y1uLQBstc19MG+FvkAtJBnuvoOMLg7mvZPvwMqbjX6PcU8eP4I5WTfSRSXbXskYImI9ysJ6FMZ6LaIQ45kILRb+MGOBR0AkEtsgBCB7nkJaVYLr5yQAELLuwBCmnW9K1cssjvTjLMr1iyGXYmnw1fh6araZskn3Jeng9jybQw8SFxEMMatk8+pPZ1Zc7sS3DjXgZuTRUjXtTAhnk8wQb5Pzosas3ZW1EWmyHTl9RN0833YSG5pXPw7sulYSWTY7xB2yYgpBGYz+QG6ZmgLu2TEFAKzmborV90UdsmIY6643Rt09Aat3vnLxP9UkJDPXHuo3tTunt1EGdMFI3kYzRRBjLoEISCn9v5KxqkrXnUnA1ulCTUlXaWFOmbRVq2juzqRsYJ7ujwhb7vqEyvUT9rgtg6rwHc7yxOe6YpKWf9Jg1nevA7nf+jpqPBTn+X73Vaq6B/p+kn7ha2HLcdWnx50gVMecs/k+8taydWBhlwHaEDL2McWtYjx6gRZ8kSNAbIxwsRrrezUqPlYIfRyUFOa48Jh47R9BEYyvGAX+I7rQCCTxONQ5HptOBILewhD5OdAQvi8MIKXCaN30fG5ZTriWk4LHo4Fbdf1AHFlZoKRdxw8YOulC4KfGprMcu/w/ocUfKeCbeirAZWvs8nlnk0mEzzxRh87m4zGADm/z9mEptHjXEO7l0OKUWSwyQkPKfJ2922OwgfsvhODbv8F&lt;/diagram&gt;&lt;/mxfile&gt;&quot;}"></div>
&lt;그림 1. i-mind 클라우드 아키텍처&gt;
{: style="text-align: center;"}

위와 같이 Event-driven 아키텍처를 구성한 이유는 다음과 같았습니다.

- 실시간 전송
    - 실시간으로 발생하는 이벤트에 대해 처리가 이루어져야 함
- 비동기 통신
    - 사용자는 요청을 보내고 결과를 바로 받을 필요가 없음
        - 영상 처리를 요청하고 이에 대한 메타 데이터와 생성된 보고서 확인은 나중에 함
- 서버 가용성
    - 사용자 수의 증가로 인한 요청 증가에도 Scale-out으로 유연하게 처리 가능

### 부족했던 점
API Gateway 뒤에 바로 Lambda를 연동하면 API 요청 수신에 장애가 생기거나 Lambda 인스턴스 생성에 문제가 발생할 경우 이를 처리할 수 있는 방법, 즉 전달된 요청 메세지가 잘 처리됐는지 확인하고 에러 발생 시 이에 대처할 수 있는 메세지 큐 시스템을 활용해야 할 필요가 있습니다. 그리고 이런 개념은 어플리케이션 레벨에서 멀티스레드의 자원 한계를 극복하기 위한 비동기 처리 패턴인 Event-driven 패턴(Event-queue, Event-loop, Thread pool 이용)에서도 사용되는 개념입니다. 어플리케이션 레벨에서 이런 메세지 큐 시스템을 담당하는 미들웨어를 메세지 브로커라고 하며 RabbitMQ나 Kafka 등이 있습니다. 그리고 AWS 클라우드에서는 메세지 브로커 서비스로 AWS SQS를 제공하고 있습니다. 프로젝트를 진행할 때는 이런 부분을 고려하지 않고 설계했던 것이 다소 아쉬운 점으로 남아있습니다. 결과적으로 다음과 같은 아키텍처를 설계하는 것이 더 바람직하다고 생각이 듭니다.

<div class="mxgraph" style="max-width:100%; margin:auto;" data-mxgraph="{&quot;highlight&quot;:&quot;#0000ff&quot;,&quot;lightbox&quot;:false,&quot;nav&quot;:true,&quot;edit&quot;:&quot;_blank&quot;,&quot;xml&quot;:&quot;&lt;mxfile host=\&quot;app.diagrams.net\&quot; modified=\&quot;2022-06-26T13:03:43.821Z\&quot; agent=\&quot;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36\&quot; etag=\&quot;oR6V0CG-e5s9ejdAK2VQ\&quot; version=\&quot;20.0.3\&quot; type=\&quot;google\&quot;&gt;&lt;diagram id=\&quot;CekXfJvFp_c8sRQF852G\&quot; name=\&quot;AWS Architecture advanced\&quot;&gt;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&lt;/diagram&gt;&lt;/mxfile&gt;&quot;}"></div>
&lt;그림 2. i-mind 클라우드 아키텍처(개선)&gt;
{: style="text-align: center;"}

위 아키텍처에서는 기능 확장 시 추가로 SQS를 생성하고 Lambda와 같은 Event Processor가 추가된 SQS를 구독하도록 할 수 있어 기능 확장 측면에서도 유연하고 내구성 있는 아키텍처라고 생각이 듭니다.

## 프로파일링 도구와 성능 저하 원인 파악, 배치 처리를 통한 해결
i-mind 서비스의 AI 기능을 담당하는 [i-mind-net](https://github.com/DrMaemi/i-mind-net)은 Vision AI 기술 중 Object Detector & Tracker, Action Detector, Human Re-identification, Emotion Detector 등 다양한 다양한 모델들을 사용해서 영상 내 사람의 위치, 행동, 표정 데이터를 주기적으로 추출하는 기능을 수행합니다. 이렇게 다양한 모델들을 사용해야 하다 보니 기능을 모듈화하고 연동하는데 성능 하락 이슈가 발생할 것이라 예측했고 실제로 1분 영상을 처리하는데 처음엔 5분 이상 걸렸었습니다. 이는 서비스에서 10분 내외의 영상을 입력받았을 때 상담 보고서 생성에 필요한 데이터들을 추출해내는데 50분 이상 걸린다는 의미였습니다.

이 문제를 해결할 수 있는 방법을 모색하다가, SW의 모듈별 성능 테스트를 위한 도구로 프로파일링 도구를 사용한다는 것을 알게 됐고 언어별로 C++, Java, Python에 사용되는 프로파일링 도구가 존재한다는 것을 알게 됐습니다. i-mind-net은 Python으로 구현되어 있었기에 Python 프로파일링 도구인 cProfile, Palanteer, Pyinstrument 등을 찾아보다가 파이썬 표준 라이브러리에 포함되어 있고 사용법이 비교적 간단한 cProfile을 사용하기로 했습니다.

```python:i-mind-net/demo/demo.py
import cProfile, pstats, io

if __name__ == "__main__":
    pr = cProfile.Profile()
    pr.enable()
    main()
    s = io.stringIO()
    sortby = pstats.sortKey.CUMULATIVE
    ps = pstats.Stats(pr, stream=s).sort_stats(sortby)
    ps.print_stats()
    print(s.getValue())
```

<p class=short>실행 로그</p>

```txt
$ python demo.py --video-path ~/dev/data/videos/legacy/실외축구_1분.mp4 --output-path ../data/output/실외축구_1분.mp4 --reid --show-id
Successfully loaded pretrained weights from "model_data/models/model.pth"
** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']
Building train transforms ...
+ resize to 256x128
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
Building test transforms ...
+ resize to 256x128
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
ReID model loaded
Starting video demo, video path: /home/drmaemi/dev/data/videos/legacy/실외축구_1분.mp4
Loading action model weight from ../data/models/aia_models/resnet101_8x8f_denseserial.pth.
Action model weight successfully loaded.
Loading YOLO model..
Network successfully loaded
YOLOv5+DeepSORT Network successfully loaded
Total 1500 frames
Showing tracking progress bar (in fps). Other processes are running in the background.
Tracker Progress: 100%|███████████▉| 1500/1501 [03:29<00:00,  7.16 frame/s]
Tracking finished. Showing feature extraction progress bar [ ready length / total length ](in msec).
Feature Extraction: 0it [00:00, -489877.12it/s]
Feature extraction finished. Now showing action prediction progress bar [ ready point count / total prediction point ]
Action Prediction: 100%|███████████████████| 48/48 [00:00<00:00, 34.15it/s]
Action prediction is done.
Final ids and their sub-ids: {1: [1, 20, 107, 111, 146, 159, 175], 2: [2, 14, 46, 154, 188], 3: [3, 18, 110, 142, 171, 173, 176, 179, 189], 4: [4, 28, 58, 71, 75, 170, 183], 5: [5, 11, 17, 30, 55, 67, 69, 84, 143], 60: [60, 96, 100, 106, 158, 169], 64: [64, 90, 98, 157, 163]}
Re-ID took 45.384 seconds
Showing video writer progress (in fps).
Action Visualizer: 100%|███████████| 1500/1500 [00:45<00:00, 32.57 frame/s]
The output video has been written to the disk.
AVA predictor worker terminated.
[W CudaIPCTypes.cpp:90] Producer process tried to deallocate over 1000 memory blocks referred by consumer processes. Deallocation might be significantly slowed down. We assume it will never going to be the case, but if it is, please file but to https://github.com/pytorch/pytorch
         53717574 function calls (51506885 primitive calls) in 329.890 seconds
```

로그를 살펴보면 Model Loading → Tracker Progress → Action Prediction → Visualizing 순으로 과정이 진행됨을 알 수 있는데, 저는 Tracker Progress에서 소요 시간이 지나치게 길다는 점에 유의했습니다. 그런데 문제는 단순 Progress bar와 소요 시간만으로는 정확히 어느 지점에서 성능 하락이 발생하는지 알 수 없었습니다. 하지만 프로파일링 도구를 통해서 커널의 built-in API 수준으로 실행 시간을 측정할 수 있었습니다.

<p class=short>cProfile 결과 로그</p>

```txt
   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1   21.076   21.076  329.896  329.896 demo.py:42(main)
      706    0.423    0.001  164.480    0.233 /home/drmaemi/dev/github/repos/i-mind-net/demo/reid.py:59(_features)
     6509    0.079    0.000  151.510    0.023 /home/drmaemi/dev/github/repos/i-mind-net/demo/reid.py:42(_extract_features)
1197877/19528   19.222    0.000  122.275    0.006 /home/drmaemi/anaconda3/envs/i-mind-net-github/lib/python3.7/site-packages/torch/nn/modules/module.py:715(_call_impl)
     6509    0.097    0.000  116.479    0.018 /home/drmaemi/dev/github/repos/i-mind-net/torchreid/models/resnet.py:353(forward)
     6509    0.395    0.000  115.565    0.018 /home/drmaemi/dev/github/repos/i-mind-net/torchreid/models/resnet.py:342(featuremaps)
52080/26044    0.874    0.000  110.925    0.004 /home/drmaemi/anaconda3/envs/i-mind-net-github/lib/python3.7/site-packages/torch/nn/modules/container.py:115(forward)
   104144    7.752    0.000  107.665    0.001 /home/drmaemi/dev/github/repos/i-mind-net/torchreid/models/resnet.py:134(forward)
     4550    0.097    0.000   75.773    0.017 /home/drmaemi/anaconda3/envs/i-mind-net-github/lib/python3.7/multiprocessing/queues.py:91(get)
     3338    0.041    0.000   72.459    0.022 /home/drmaemi/anaconda3/envs/i-mind-net-github/lib/python3.7/multiprocessing/connection.py:208(recv_bytes)
...
```

결과를 보면 Cumulative time을 기준으로 내림차순 정렬이 되어있는 것을 확인할 수 있고 실행 시간이 가장 오래 걸린 TOP 10 로그만을 살펴봤습니다. Tracker Progress에서 영상 내 객체 재식별(화면 바깥으로 나갔다가 다시 들어온 경우나, 사물/사람에 의해 가려져 추적에 실패한 경우 다시 추적을 재개할 수 있도록 하는)을 위한 특징 벡터 추출에 해당하는 메서드인 `_features`, `_extract_features`에서 프로세싱의 과정 전체 329초 중 164초를 사용했음을 알 수 있었는데, 당시 코드는 다음과 같았습니다.

```python:reid.py
class REID:
    def __init__(self):
        self.model = torchreid.models.build_model(
                name='resnet50',
                num_classes=1,#human
                loss='softmax',
                pretrained=True,
                use_gpu = True
            )
        torchreid.utils.load_pretrained_weights(self.model, 'model_data/models/model.pth')
        self.model = self.model.cuda()
        self.optimizer = torchreid.optim.build_optimizer(
                self.model,
                optim='adam',
                lr=0.0003
            )
        self.scheduler = torchreid.optim.build_lr_scheduler(
                self.optimizer,
                lr_scheduler='single_step',
                stepsize=20
            )
        _, self.transform_te = build_transforms(
            height=256, width=128,
            random_erase=False,
            color_jitter=False,
            color_aug=False
        )
        self.dist_metric = 'euclidean'
        self.model.eval()

    def _extract_features(self, input):
        self.model.eval()
        return self.model(input)

    def _features(self, imgs):
        f = []
        for img in imgs:
            img = Image.fromarray(img.astype('uint8')).convert('RGB')
            img = self.transform_te(img)
            img = torch.unsqueeze(img, 0)
            img = img.cuda()
            features = self._extract_features(img)
            features = features.data.cpu() #tensor shape=1x2048
            f.append(features)
        f = torch.cat(f, 0)
        return f

    def compute_distance(self, qf, gf):
        distmat = metrics.compute_distance_matrix(qf, gf, self.dist_metric)
        # print(distmat.shape)
        return distmat.numpy()
```

main 함수에서 Object Detector로 사람 위치를 표시한 경계 상자(Boundary box) 이미지를 배치 처리로 k개 이미지를 reid 모듈에 넘기면 `_features` 함수 내부에서 k개 각각을 GPU로 넘기게 되는데 CPU-GPU 간 Context switch가 너무 잦은게 성능 저하의 원인일 것이라 추측했습니다. 이를 해결하는 방법은 `_features` 함수에서 k개 이미지를 각각 처리하여 CPU 메모리로 넘기는 것이 아니라, main 함수의 배치 처리와 같이 k개 전체를 GPU로 넘겨 한 번에 처리한 뒤 CPU 메모리로 옮겨 문맥 교환 빈도를 최소화하는 것이라고 생각했고, 다음과 같이 코드를 수정했습니다.

```python:reid.py
class REID:
    ...

    def _features(self, imgs):
        for i in range(len(imgs)):
            imgs[i] = Image.fromarray(imgs[i].astype('uint8')).convert('RGB')
            imgs[i] = self.transform_te(imgs[i])
            imgs[i] = torch.unsqueeze(imgs[i], 0)
            imgs[i] = imgs[i].cuda()
        ft_imgs = torch.cat(imgs, 0)
        f = self._extract_features(ft_imgs)
        del ft_imgs
        torch.cuda.empty_cache()
        f = f.data.cpu()
        return f
    
    ...
```

이후 다시 실행하여 처리 성능을 확인해봤는데 극적으로 성능이 향상됐음을 알 수 있었습니다.

<p class=short>배치 처리 수정 후 실행 로그</p>

```txt
python demo.py --video-path ~/dev/data/videos/legacy/실외축구_1분.mp4 --output-path ../data/output/실외축구_1분.mp4 --reid --show-id
Successfully loaded pretrained weights from "model_data/models/model.pth"
** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']
Building train transforms ...
+ resize to 256x128
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
Building test transforms ...
+ resize to 256x128
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
ReID model loaded
Starting video demo, video path: /home/drmaemi/dev/data/videos/legacy/실외축구_1분.mp4
Loading action model weight from ../data/models/aia_models/resnet101_8x8f_denseserial.pth.
Action model weight successfully loaded.
Loading YOLO model..
Network successfully loaded
YOLOv5+DeepSORT Network successfully loaded
Total 1500 frames
Showing tracking progress bar (in fps). Other processes are running in the background.
Tracker Progress: 100%|███████████▉| 1500/1501 [01:30<00:00, 16.67 frame/s]
Tracking finished. Showing feature extraction progress bar [ ready length / total length ](in msec).
Feature Extraction: 49200it [00:00, ?it/s]Feature extraction finished. Now showing action prediction progress bar [ ready point count / total prediction point ]
Feature Extraction: 0it [00:00, -489741.10it/s]    | 18/48 [00:00<?, ?it/s]
Action Prediction: 100%|███████████████████| 48/48 [00:00<00:00, 44.97it/s]
Action prediction is done.
Final ids and their sub-ids: {1: [1, 20, 107, 111, 146, 159, 175], 2: [2, 14, 46, 154, 188], 3: [3, 18, 110, 142, 171, 173, 176, 179, 189], 4: [4, 28, 58, 71, 75, 170, 183], 5: [5, 11, 17, 30, 55, 67, 69, 84, 143], 60: [60, 96, 100, 106, 158, 169], 64: [64, 90, 98, 157, 163]}
Re-ID took 42.263 seconds
Showing video writer progress (in fps).
Action Visualizer: 100%|███████████| 1500/1500 [00:46<00:00, 31.92 frame/s]
The output video has been written to the disk.
AVA predictor worker terminated.
[W CudaIPCTypes.cpp:90] Producer process tried to deallocate over 1000 memory blocks referred by consumer processes. Deallocation might be significantly slowed down. We assume it will never going to be the case, but if it is, please file but to https://github.com/pytorch/pytorch
         25845710 function calls (25579026 primitive calls) in 201.752 seconds
```

기존 실행 시간 329초에서 201초까지 시간을 약 40% 단축할 수 있었습니다. cProfile 로그 결과도 한 편 살펴봅시다.

```txt
   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1   20.035   20.035  201.759  201.759 demo.py:42(main)
     4550    0.095    0.000   76.265    0.017 /home/drmaemi/anaconda3/envs/i-mind-net-github/lib/python3.7/multiprocessing/queues.py:91(get)
     3338    0.040    0.000   72.962    0.022 /home/drmaemi/anaconda3/envs/i-mind-net-github/lib/python3.7/multiprocessing/connection.py:208(recv_bytes)
     3338    0.055    0.000   72.903    0.022 /home/drmaemi/anaconda3/envs/i-mind-net-github/lib/python3.7/multiprocessing/connection.py:406(_recv_bytes)
     6676    0.641    0.000   72.821    0.011 /home/drmaemi/anaconda3/envs/i-mind-net-github/lib/python3.7/multiprocessing/connection.py:374(_recv)
   113180   68.872    0.001   68.872    0.001 {built-in method posix.read}
        1    0.017    0.017   46.936   46.936 /home/drmaemi/dev/github/repos/i-mind-net/demo/visualizer.py:502(progress_bar)
      706    0.216    0.000   36.136    0.051 /home/drmaemi/dev/github/repos/i-mind-net/demo/reid.py:90(_features)
     1501    0.014    0.000   29.692    0.020 /home/drmaemi/dev/github/repos/i-mind-net/demo/action_predictor.py:332(read_track)
     6509   26.068    0.004   26.246    0.004 {method 'add_item' of 'annoy.Annoy' objects}
```

가장 시간이 오래 걸렸던 `_features` 함수가 어디갔나 살펴봤더니 TOP2에서 TOP8까지 내려간 모습입니다. 기존 164초 수행 시간에서 36초로 단축된 모습입니다. 프로파일링 도구를 사용하지 않았다면 원인 파악부터 성능 측정까지 많은 개발 비용이 들었을 것 같은데 금번 프로젝트를 통해서 성능 모니터링이나 프로파일링의 중요성을 깨닫고, 나아가 테스트 코드가 왜 필요한지 절실히 느낄 수 있었습니다.

### 부족한 점/아쉬웠던 점
위와 같은 상황을 미연에 방지하기 위해, 그리고 기능 확장과 단위 성능 테스트를 진행하기 위해 모듈화를 진행하는데 본 프로젝트에서는 AI 기능의 각 요소인 Object Detector, Tracker, Re-ID, Action Detector, Emotion Detector들을 효과적으로 모듈화하지 못했던 점이 아쉬웠습니다. 컴퓨터공학을 전공하면서 수행해봤던 가장 큰 규모의 프로젝트임에도 불구하고 대규모 프로젝트에 반드시 필요한 모듈화, 테스트 코드 작성 등을 제대로 하지 못했고 이에 대해 스스로 반성했습니다.

또, 배치 처리를 통한 문제 해결 시 고려해야 될 문제로 배치 사이즈를 어떻게 측정하는가에 대해 제대로 된 테스트를 진행하지 못했습니다. 배치 사이즈와 성능 향상은 서로 비례하는데 지나치게 큰 배치 사이즈는 런타임 중 OOM(Out Of Memory) 에러를 발생시키기 때문에 별도 테스트나 배치 사이즈를 결정하는 알고리즘이 필요한데 이런 부분에 대한 고도화를 진행하진 못했습니다.

## A. 참조
12bme, "[아키텍처] 이벤트 주도 아키텍처(Event-Driven Architecture)", *Tistory*, Dec. 29, 2019. [Online]. Available: [https://12bme.tistory.com/540](https://12bme.tistory.com/540) [Accessed Jun. 26, 2022].

Wonit, "[마이크로서비스] MSA의 핵심 구성 요소 - Message Queueing", *Tistory*, Apr. 14, 2021. [Online]. Available: [https://wonit.tistory.com/491](https://wonit.tistory.com/491) [Accessed Jun. 26, 2022].

Wonit, "[데이터베이스] Isolation Level, 고립 수준", *Tistory*, Apr. 7, 2021. [Online]. Available: [https://wonit.tistory.com/463?category=790502](https://wonit.tistory.com/463?category=790502) [Accessed Jun. 26, 2022].
